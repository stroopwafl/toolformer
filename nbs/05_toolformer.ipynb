{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aea6d91-cbb6-4631-b51f-ecbb24195499",
   "metadata": {},
   "source": [
    "# Toolformer - dataset filtering\n",
    "\n",
    "Main bulk of toolformer functions for building a dataset for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66199e-92ba-4243-bafb-3553bda9860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b91393a-084f-435e-a6b4-156b58fc3248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import math, random, torch, matplotlib.pyplot as plt, numpy as np, matplotlib as mpl, shutil, os, gzip, pickle, re, copy, time\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import fastcore.all as fc\n",
    "from glob import glob\n",
    "\n",
    "from torch import tensor, nn, optim\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from torch.nn import init\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import List, Optional\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "from fastprogress import progress_bar\n",
    "from einops import rearrange\n",
    "\n",
    "from toolformer.datasets import *\n",
    "from toolformer.tokenizer import *\n",
    "from toolformer.model import *\n",
    "from toolformer.tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85b41f-1a8c-47f2-ba84-3e849bcc93f3",
   "metadata": {},
   "source": [
    "We are going to use in-context learning to finetune the model. We'll start with a prompt that teaches the model how to use a tool, and build a dataset of examples which vary the final input value inside this prompt. This first involves choosing a token to represent the beginning and end of an instance of tool usage.\n",
    "\n",
    "Through trial and error, I chose \"<%\" and \"%>\" because these were the shortest tokens I could find that were a) represented by a single token, b) represented only once in the vocabulary (i.e. there are no duplicates) and c) unlikely to come up otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c36d536-1cf1-4d9a-8b73-e1ab40b0ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/models/foundation/LLaMA/tokenizer.model'\n",
    "tokenizer = Tokenizer(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a8baa5-fb91-49c1-b95e-1dfc73ad228c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{$ : 2\n",
      "{: : 2\n",
      "!> : 0\n",
      "<! : 1\n",
      "<% : 1\n",
      "%> : 1\n"
     ]
    }
   ],
   "source": [
    "p = ['{$', '{:', '!>', '<!', '<%', '%>']\n",
    "\n",
    "for a in p:\n",
    "    counter = 0\n",
    "    for i in range(32000):\n",
    "        t = tokenizer.decode(i)\n",
    "        if t == a: counter += 1\n",
    "    print(f'{a} : {counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ff9cb-832d-4ede-90ba-ff00d89ae277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([20577], [6580])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<%', False, False), tokenizer.encode('%>', False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791da664-5e27-4f6a-a61e-05d2b47f2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    'Output: The number in the next term is 18 + 12 x 3 = 54.',\n",
    "    'Output: From this, we have 4 * 30 minutes = <% Calculator(4 * 30) %> 120 minutes. ',\n",
    "    'Output: From this, <% Calculator(4 * 30) %> we have 4 * 30 minutes = <% Calculator(3 * 50) %> 120 minutes. <% Calculator(9 * 80) %>',\n",
    "    'Output: Today is the first <% Calendar() %> Friday of the year.',\n",
    "    'Output: The president of the United States is <% Calendar() %> Joe Biden.'\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637fb96-e7a1-4c61-94d2-b22ee3a3e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def filter_and_retain_only_first_api(prompts:List[str], api_start_char:str, api_end_char:str, start_idxs:Optional[List[int]]=None):\n",
    "    \"\"\"\n",
    "        Takes a list of strings and attempts to find instances of API calls in the \n",
    "        pattern <start_char> api(expression) <end_char>, and returns the original strings where only the\n",
    "        first instance of this pattern remains (all others are replaced with \"\"). If \n",
    "        no matches are found, discards the string.\n",
    "    \"\"\"\n",
    "    prompts_with_api_calls, indexes = [], []\n",
    "    s,e = api_start_char, api_end_char\n",
    "    if start_idxs is None: start_idxs = [0] * len(prompts)\n",
    "    for i, (prompt, idx) in enumerate(zip(prompts, start_idxs)):\n",
    "        p = prompt[idx:]\n",
    "        try:\n",
    "            matches = re.findall(f'(?<=\\s){s}\\s.*?\\s{e}(?=\\s?)', p)\n",
    "            if len(matches) >= 1:\n",
    "                if len(matches) > 1:\n",
    "                    for match in matches[1:]: \n",
    "                        p = p.replace(match, '', 1)\n",
    "                prompt = prompt[:idx] + p\n",
    "                prompts_with_api_calls.append(prompt)\n",
    "                indexes.append(idx)\n",
    "        except Exception: print(p)\n",
    "    return prompts_with_api_calls, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b660275-a10b-46c1-9c07-99a92401a17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: From this, we have 4 * 30 minutes = <% Calculator(4 * 30) %> 120 minutes. \n",
      "Output: From this, <% Calculator(4 * 30) %> we have 4 * 30 minutes =  120 minutes. \n",
      "Output: Today is the first <% Calendar() %> Friday of the year.\n",
      "Output: The president of the United States is <% Calendar() %> Joe Biden.\n"
     ]
    }
   ],
   "source": [
    "api_start_char, api_end_char = '<%', '%>'\n",
    "test_cases, i = filter_and_retain_only_first_api(test_cases, api_start_char, api_end_char)\n",
    "for a in test_cases: print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176512b-f254-424d-80fd-82253843766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_api_calls(results, prompts, api_start_char:str, api_end_char:str, start_idxs:List[int]=None):\n",
    "    prompts_with_responses = []\n",
    "    s,e = api_start_char, api_end_char\n",
    "    for r, prompt, i in zip(results, prompts, start_idxs):\n",
    "        p = prompt[i:]\n",
    "        call = re.search(f'{s}.*?{e}', p).group(0)\n",
    "        call_with_response = call.replace(f'{e}', '') + '→ ' + str(r) + f' {e}'\n",
    "        p = p.replace(call, call_with_response)\n",
    "        prompt = prompt[:i] + p\n",
    "        prompts_with_responses.append(prompt)\n",
    "    return prompts_with_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace2ed6-f595-4c42-bf10-3aacd174c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_api_calls(prompts:List[str], api_start_char:str, api_end_char:str, start_idxs:List[int]=None):\n",
    "    \"\"\"\n",
    "        Extracts a calculator API call in the format <start_char> api(expression) <end_char> from a string,\n",
    "        executes the API call and returns a new string including a response in the format\n",
    "        <start_char> api(expression) → response <end_char>. \n",
    "        \n",
    "        ### Example\n",
    "        \n",
    "        input: 'The number in the next term is 18 + 12 x 3 = << Calculator(18 + 12 x 3) >> 54.'\n",
    "        output: 'The number in the next term is 18 + 12 x 3 = << Calculator(18 + 12 x 3) → 54 >> 54.'\n",
    "    \"\"\"\n",
    "    results, indexes = [], []\n",
    "    s,e = api_start_char, api_end_char\n",
    "    if start_idxs is None: start_idxs = [0] * len(prompts)\n",
    "    for i, (p, idx) in enumerate(zip(prompts, start_idxs)):\n",
    "        p = p[idx:]\n",
    "        call = re.search(f'{s}.*?{e}', p).group(0)\n",
    "        func_name = re.search(f'{s}\\s*(.*?)\\(', call).group(1)\n",
    "        expression = re.search('\\((.*?)\\)', call).group(1)\n",
    "        try:\n",
    "            if func_name.lower() == \"calculator\": res = Calculator(expression)\n",
    "            elif func_name.lower() == \"calendar\": res = Calendar()\n",
    "            results.append(res)\n",
    "            indexes.append(i)\n",
    "        except Exception: pass\n",
    "    prompts, start_idxs = [prompts[i] for i in indexes], [start_idxs[i] for i in indexes]\n",
    "    prompts_with_responses = format_api_calls(results, prompts, api_start_char, api_end_char, start_idxs=start_idxs)\n",
    "    return prompts_with_responses, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c624b9-7b7d-4a4b-8f3c-54ee15ddd235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: From this, we have 4 * 30 minutes = <% Calculator(4 * 30) → 120 %> 120 minutes. \n",
      "Output: From this, <% Calculator(4 * 30) → 120 %> we have 4 * 30 minutes =  120 minutes. \n",
      "Output: Today is the first <% Calendar() → Today is Thursday, July 20, 2023. %> Friday of the year.\n",
      "Output: The president of the United States is <% Calendar() → Today is Thursday, July 20, 2023. %> Joe Biden.\n"
     ]
    }
   ],
   "source": [
    "test_cases,i = make_api_calls(test_cases, api_start_char, api_end_char)\n",
    "for a in test_cases: print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd7e5e-6fa7-4ccb-923e-abf07c23c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_probs(token_ids, logits):\n",
    "    \"\"\"\n",
    "        Calculates a probability distribution over the vocabulary for each position\n",
    "        in the input sequence (predicts the next token), and for each position, returns \n",
    "        the probability of the next actual/observed token in the input sequence.\n",
    "    \"\"\"\n",
    "    logits = logits[:, :-1]\n",
    "    token_ids = token_ids[:, 1:]\n",
    "    token_ids = rearrange(token_ids, 'b n -> b n 1')\n",
    "    probs = logits.softmax(-1)\n",
    "    correct_token_id_pred_prob = probs.gather(-1, token_ids)\n",
    "    return rearrange(correct_token_id_pred_prob, 'b n 1 -> b n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a4f14f-8f98-4f66-ac71-f0771a6ab950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def weight_func(t): return (1. - t * 0.2).clamp(min=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9684ed26-ebb3-4716-b88a-a565dcc3855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_weights(tokens, search_token_id, pad_id=-1, weight_func=weight_func, start_index=None):\n",
    "    \"\"\"\n",
    "        Searches for the search_token_id in the sequence, and produces a \n",
    "        weight vector that degrades weighting off a cliff after the \n",
    "        search_token_id. Weights returned are equal for all tokens preceding\n",
    "        the search_token_id, and grade down to 0 over the next 5 tokens.\n",
    "    \"\"\"\n",
    "    # find the api_start_token\n",
    "    is_token_id_mask = torch.zeros_like(tokens, dtype=bool)\n",
    "    rows = torch.arange(is_token_id_mask.shape[0])\n",
    "    for i in range(len(tokens)):\n",
    "        idx = start_index[i]\n",
    "        is_token_id_mask[i,idx:] = (tokens[i,idx:] == search_token_id)\n",
    "    # generate a monotonic arange for all tokens after api_start_token\n",
    "    arange = (is_token_id_mask.cumsum(dim=-1) > 0).cumsum(dim=-1)\n",
    "    # set everything before the api_start_token to 0\n",
    "    before_token_mask = arange == 0\n",
    "    # set api_start_token to 0 in range\n",
    "    arange = arange - 1\n",
    "    # replace all before api_start_token with 0, so 0 up to api_start_token + 1\n",
    "    arange = arange.masked_fill(before_token_mask, pad_id)\n",
    "    # we now have a range like [0,0,0,0,0,0,0,(api_token)0,1,2,3,4,5...]\n",
    "    weights = weight_func(arange)\n",
    "    # now we have a weight vector like [1.2,1.2,1.2,1.2,1.2,1.2,1.2,(search_token_id)1,0.8,0.6,0.4,0.2,0,0,0,0...]\n",
    "    return weights.masked_fill(weights == pad_id, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df621437-94f7-4eed-94d5-77612ac40c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def toolformer_probability_filter(tokens_without_api_calls, tokens_with_api_calls, tokens_with_api_responses, api_start_token, api_end_token, tau_filter=1., start_idxs=None, device='cuda'):\n",
    "    # get the logits\n",
    "    def add_dims(x): return x[None, :] if len(x.shape) < 2 else x\n",
    "    \n",
    "    tokens_without_api_calls, tokens_with_api_calls, tokens_with_api_responses = map(lambda t: add_dims(t).to(device), (tokens_without_api_calls, tokens_with_api_calls, tokens_with_api_responses))\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        logits, logits_with_api_calls, logits_with_api_responses = map(partial(model, start_pos=0), (tokens_without_api_calls, tokens_with_api_calls, tokens_with_api_responses))\n",
    "    \n",
    "    # get the predicted probabilities\n",
    "    probs_without_api_calls = get_probs(tokens_without_api_calls, logits)\n",
    "    probs_with_api_calls = get_probs(tokens_with_api_calls, logits_with_api_calls)\n",
    "    probs_with_api_responses = get_probs(tokens_with_api_responses, logits_with_api_responses)\n",
    "    \n",
    "    # get the weightings\n",
    "    weights_without_api_calls = get_weights(tokens_with_api_calls[:, 1:], api_start_token, start_index=tensor(start_idxs))\n",
    "    weights_with_api_calls = get_weights(tokens_with_api_calls[:, :-1], api_end_token, start_index=tensor(start_idxs))\n",
    "    weights_with_api_responses = get_weights(tokens_with_api_responses[:, :-1], api_end_token, start_index=tensor(start_idxs))\n",
    "    \n",
    "    for w in weights_without_api_calls: assert w.sum() > 0\n",
    "    \n",
    "    # calculate the loss for each version\n",
    "    def loss(weights, probs): return -(weights * probs.log()).sum(-1)\n",
    "    loss_original = loss(weights_without_api_calls, probs_without_api_calls)\n",
    "    loss_api = loss(weights_with_api_calls, probs_with_api_calls)\n",
    "    loss_response = loss(weights_with_api_responses, probs_with_api_responses)\n",
    "\n",
    "    # toolformer filtering\n",
    "    l_minus = torch.minimum(loss_original, loss_api)\n",
    "    l_plus = loss_response\n",
    "    t_mask = (l_minus - l_plus) >= tau_filter\n",
    "    return tokens_without_api_calls[t_mask], tokens_with_api_calls[t_mask], tokens_with_api_responses[t_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8c0119-7c61-428b-8448-438ddf0c0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample(model, tokenizer, prompts: List[str], max_gen_len: int, temperature: float = 0.8, top_p: float = 0.95, decode=False, make_api_calls=False, device='cuda'):\n",
    "    bsz = len(prompts)\n",
    "    params = model.params\n",
    "    assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "    prompt_tokens = [tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "\n",
    "    min_prompt_size = min([len(t) for t in prompt_tokens])\n",
    "    max_prompt_size = max([len(t) for t in prompt_tokens])\n",
    "\n",
    "    total_len = min(params.max_seq_len, max_gen_len + max_prompt_size)\n",
    "    tokens = torch.full((bsz, total_len), tokenizer.pad_id).to(device).long()\n",
    "    for k, t in enumerate(prompt_tokens):\n",
    "        tokens[k, : len(t)] = torch.tensor(t).long()\n",
    "    input_text_mask = tokens != tokenizer.pad_id\n",
    "    start_pos = min_prompt_size\n",
    "    prev_pos = 0\n",
    "    \n",
    "    for cur_pos in range(start_pos, total_len):\n",
    "        logits = model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "            next_token = sample_top_p(probs, top_p)\n",
    "        else:\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "        next_token = next_token.reshape(-1)\n",
    "        # only replace token if prompt has already been generated\n",
    "        next_token = torch.where(\n",
    "            input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "        )\n",
    "        tokens[:, cur_pos] = next_token\n",
    "        prev_pos = cur_pos\n",
    "    return tokens if not decode else decode_tokens(tokenizer, tokens, prompt_tokens, max_gen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d178351-3d80-4304-8adc-77ce847eb6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad()\n",
    "def build_finetune_dataset(dataloader, model, tokenizer, api_start_char='<$', api_end_char='%>', return_tokens=True, device='cuda'):\n",
    "    \"\"\"\n",
    "        Samples API calls using in-context learning, and returns a dataset\n",
    "        that contains only examples for which calling the API increased the \n",
    "        model's ability to predict the next token.\n",
    "    \"\"\"\n",
    "    finetune_data = []\n",
    "    model = model.to(device)\n",
    "    api_start_token = tokenizer.encode(api_start_char, False, False)[0]\n",
    "    api_end_token = tokenizer.encode(api_end_char, False, False)[0]\n",
    "    for it, batch in enumerate(progress_bar(dataloader, leave=False)):\n",
    "\n",
    "        # assemble the null prompts assuming no API calls\n",
    "        prompts, (data_without_api_calls, start_idxs) = batch\n",
    "\n",
    "        data_without_api_calls = [p + d for p,d in zip(prompts, data_without_api_calls)]\n",
    "\n",
    "        # generate samples with possible API calls, and filter to a single API call per prompt\n",
    "        sampled_prompts = sample(model, tokenizer, prompts, max_gen_len=100, decode=True, device=device)\n",
    "        data_with_api_calls, indexes = filter_and_retain_only_first_api(sampled_prompts, api_start_char, api_end_char, start_idxs)\n",
    "        if len(data_with_api_calls) == 0: continue\n",
    "\n",
    "        # make the api calls\n",
    "        try: data_with_api_responses, indexes = make_api_calls(data_with_api_calls, api_start_char, api_end_char, indexes)\n",
    "        except Exception: continue\n",
    "        if len(data_with_api_responses) == 0: continue\n",
    "\n",
    "        # retain only data where we have a) without call, b) with call and c) with response\n",
    "        data_with_api_calls = [data_with_api_calls[i] for i in indexes]\n",
    "        data_without_api_calls = [data_without_api_calls[i] for i in indexes]\n",
    "\n",
    "        # convert to tokens and pad to same length\n",
    "        to_tokens = lambda l: pad_sequence(encode_to_tensor(tokenizer, l), batch_first=True)\n",
    "        tokens_without_api_calls, tokens_with_api_calls, tokens_with_api_responses = map(\n",
    "            to_tokens, (data_without_api_calls, data_with_api_calls, data_with_api_responses)\n",
    "        )\n",
    "        tokens_without_api_calls, tokens_with_api_calls, tokens_with_api_responses = torch.chunk(\n",
    "            pad_sequence(\n",
    "                [j for i in [tokens_without_api_calls, tokens_with_api_calls, tokens_with_api_responses] for j in i], \n",
    "                batch_first=True\n",
    "        ), 3, dim=0)\n",
    "\n",
    "        # filter data via the main toolformer equation\n",
    "        token_start_idxs = [encode_to_tensor(tokenizer, p).shape[-1] for p in prompts]\n",
    "        token_start_idxs = [token_start_idxs[i] for i in indexes]\n",
    "        finetune_tokens, finetune_tokens_with_api_calls, finetune_tokens_with_api_responses = toolformer_probability_filter(\n",
    "            tokens_without_api_calls, tokens_with_api_calls, tokens_with_api_responses, api_start_token, api_end_token, start_idxs=token_start_idxs, device=device\n",
    "        )\n",
    "\n",
    "        # store the relevant data\n",
    "        if len(finetune_tokens_with_api_calls) >= 1: \n",
    "            for f in finetune_tokens_with_api_calls: finetune_data.append(f.cpu())\n",
    "\n",
    "    if return_tokens: return finetune_data\n",
    "    prompts = []\n",
    "    for f in finetune_data:\n",
    "        l = [i.item() for i in f if not i == 0]\n",
    "        prompts.append(tokenizer.decode(l))\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e6885-405d-494f-a70e-c7d8320497fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37868c34-8ded-4cfc-8664-49eb5bb74424",
   "metadata": {},
   "source": [
    "### Test (7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb18712-55f9-48ca-9592-9758616c87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7bfa2-2b6a-4902-abb2-2fa0e6dd040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['LOCAL_RANK'] = '0'\n",
    "# os.environ['WORLD_SIZE'] = '1'\n",
    "# os.environ['RANK'] = '0'\n",
    "# os.environ['MASTER_ADDR'] = '172.17.0.7'\n",
    "# os.environ['MASTER_PORT'] = '6006'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68267304-5d0b-4f11-a8b0-35409a6726df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_rank, world_size = setup_model_parallel()\n",
    "# path = '/home/models/foundation/LLaMA/7B'\n",
    "# checkpoint = torch.load(f'{path}/consolidated.00.pth')\n",
    "# with open(Path(path) / \"params.json\", \"r\") as f: params = json.loads(f.read())\n",
    "# model_args = ModelArgs(max_seq_len=2048, max_batch_size=8, **params)\n",
    "# model_args.vocab_size = tokenizer.n_words\n",
    "# model = Transformer(model_args).cuda().half()\n",
    "# torch.set_default_tensor_type(torch.FloatTensor)\n",
    "# model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383bee8-1640-451f-99db-bc109ad80f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = []\n",
    "# with open('../data/dataset.csv', 'r') as file: \n",
    "#     reader = csv.reader(file)\n",
    "#     for row in reader: d.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc6996-ec72-4daa-9a50-089464927583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = PromptDS(d)\n",
    "# dl = DataLoader(ds, batch_size=8, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d218e-0fbe-4bee-8fc3-7e2f6de75812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = build_finetune_dataset(dl, model, tokenizer, return_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff0e47-8eb9-4231-9c92-248a5913c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/libs/toolformer/data/finetune_dataset.csv', 'w', newline='') as file: \n",
    "#     writer = csv.writer(file)\n",
    "#     for d in data: writer.writerow(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "main_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
