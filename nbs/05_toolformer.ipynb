{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aea6d91-cbb6-4631-b51f-ecbb24195499",
   "metadata": {},
   "source": [
    "# Toolformer - dataset filtering\n",
    "\n",
    "Main bulk of toolformer functions for building a dataset for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66199e-92ba-4243-bafb-3553bda9860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b91393a-084f-435e-a6b4-156b58fc3248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import math, random, torch, matplotlib.pyplot as plt, numpy as np, matplotlib as mpl, shutil, os, gzip, pickle, re, copy, time\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import fastcore.all as fc\n",
    "from glob import glob\n",
    "\n",
    "from torch import tensor, nn, optim\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from torch.nn import init\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import List\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "from fastprogress import progress_bar\n",
    "from einops import rearrange\n",
    "\n",
    "from toolformer.datasets import *\n",
    "from toolformer.tokenizer import *\n",
    "from toolformer.model import *\n",
    "from toolformer.tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85b41f-1a8c-47f2-ba84-3e849bcc93f3",
   "metadata": {},
   "source": [
    "We are going to use in-context learning to finetune the model. We'll start with a prompt that teaches the model how to use a tool, and build a dataset of examples which vary the final input value inside this prompt. This first involves choosing a token to represent the beginning and end of an instance of tool usage.\n",
    "\n",
    "Through trial and error, I chose \"<%\" and \"%>\" because these were the shortest tokens I could find that were a) represented by a single token, b) represented only once in the vocabulary (i.e. there are no duplicates) and c) unlikely to come up otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c36d536-1cf1-4d9a-8b73-e1ab40b0ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/models/foundation/LLaMA/tokenizer.model'\n",
    "tokenizer = Tokenizer(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a8baa5-fb91-49c1-b95e-1dfc73ad228c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{$ : 2\n",
      "{: : 2\n",
      "!> : 0\n",
      "<! : 1\n",
      "<% : 1\n",
      "%> : 1\n"
     ]
    }
   ],
   "source": [
    "p = ['{$', '{:', '!>', '<!', '<%', '%>']\n",
    "\n",
    "for a in p:\n",
    "    counter = 0\n",
    "    for i in range(32000):\n",
    "        t = tokenizer.decode(i)\n",
    "        if t == a: counter += 1\n",
    "    print(f'{a} : {counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ff9cb-832d-4ede-90ba-ff00d89ae277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([20577], [6580])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<%', False, False), tokenizer.encode('%>', False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791da664-5e27-4f6a-a61e-05d2b47f2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    'Output: The number in the next term is 18 + 12 x 3 = 54.',\n",
    "    'Output: From this, we have 4 * 30 minutes = <% Calculator(4 * 30) %> 120 minutes. ',\n",
    "    'Output: From this, <% Calculator(4 * 30) %> we have 4 * 30 minutes = <% Calculator(3 * 50) %> 120 minutes. <% Calculator(9 * 80) %>',\n",
    "    'Output: Today is the first <% Calendar() %> Friday of the year.',\n",
    "    'Output: The president of the United States is <% Calendar() %> Joe Biden.'\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637fb96-e7a1-4c61-94d2-b22ee3a3e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def filter_and_retain_only_first_api(prompts:List[str], api_start_char:str, api_end_char:str, start_idxs:Optional[List[int]]=None):\n",
    "    \"\"\"\n",
    "        Takes a list of strings and attempts to find instances of API calls in the \n",
    "        pattern <start_char> api(expression) <end_char>, and returns the original strings where only the\n",
    "        first instance of this pattern remains (all others are replaced with \"\"). If \n",
    "        no matches are found, discards the string.\n",
    "    \"\"\"\n",
    "    prompts_with_api_calls, indexes = [], []\n",
    "    s,e = api_start_char, api_end_char\n",
    "    if start_idxs is None: start_idxs = [0] * len(prompts)\n",
    "    for i, (prompt, idx) in enumerate(zip(prompts, start_idxs)):\n",
    "        p = prompt[idx:]\n",
    "        try:\n",
    "            matches = re.findall(f'(?<=\\s){s}\\s.*?\\s{e}(?=\\s?)', p)\n",
    "            if len(matches) >= 1:\n",
    "                if len(matches) > 1:\n",
    "                    for match in matches[1:]: \n",
    "                        p = p.replace(match, '', 1)\n",
    "                prompt = prompt[:idx] + p\n",
    "                prompts_with_api_calls.append(prompt)\n",
    "                indexes.append(idx)\n",
    "        except Exception: print(p)\n",
    "    return prompts_with_api_calls, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b660275-a10b-46c1-9c07-99a92401a17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: From this, we have 4 * 30 minutes = <% Calculator(4 * 30) %> 120 minutes. \n",
      "Output: From this, <% Calculator(4 * 30) %> we have 4 * 30 minutes =  120 minutes. \n",
      "Output: Today is the first <% Calendar() %> Friday of the year.\n",
      "Output: The president of the United States is <% Calendar() %> Joe Biden.\n"
     ]
    }
   ],
   "source": [
    "api_start_char, api_end_char = '<%', '%>'\n",
    "test_cases, i = filter_and_retain_only_first_api(test_cases, api_start_char, api_end_char)\n",
    "for a in test_cases: print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176512b-f254-424d-80fd-82253843766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_api_calls(results, prompts, api_start_char:str, api_end_char:str, start_idxs:List[int]=None):\n",
    "    prompts_with_responses = []\n",
    "    s,e = api_start_char, api_end_char\n",
    "    for r, prompt, i in zip(results, prompts, start_idxs):\n",
    "        p = prompt[i:]\n",
    "        call = re.search(f'{s}.*?{e}', p).group(0)\n",
    "        call_with_response = call.replace(f'{e}', '') + '→ ' + str(r) + f' {e}'\n",
    "        p = p.replace(call, call_with_response)\n",
    "        prompt = prompt[:i] + p\n",
    "        prompts_with_responses.append(prompt)\n",
    "    return prompts_with_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace2ed6-f595-4c42-bf10-3aacd174c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_api_calls(prompts:List[str], api_start_char:str, api_end_char:str, start_idxs:List[int]=None):\n",
    "    \"\"\"\n",
    "        Extracts a calculator API call in the format <start_char> api(expression) <end_char> from a string,\n",
    "        executes the API call and returns a new string including a response in the format\n",
    "        <start_char> api(expression) → response <end_char>. \n",
    "        \n",
    "        ### Example\n",
    "        \n",
    "        input: 'The number in the next term is 18 + 12 x 3 = << Calculator(18 + 12 x 3) >> 54.'\n",
    "        output: 'The number in the next term is 18 + 12 x 3 = << Calculator(18 + 12 x 3) → 54 >> 54.'\n",
    "    \"\"\"\n",
    "    results, indexes = [], []\n",
    "    s,e = api_start_char, api_end_char\n",
    "    if start_idxs is None: start_idxs = [0] * len(prompts)\n",
    "    for i, (p, idx) in enumerate(zip(prompts, start_idxs)):\n",
    "        p = p[idx:]\n",
    "        call = re.search(f'{s}.*?{e}', p).group(0)\n",
    "        func_name = re.search(f'{s}\\s*(.*?)\\(', call).group(1)\n",
    "        expression = re.search('\\((.*?)\\)', call).group(1)\n",
    "        try:\n",
    "            if func_name.lower() == \"calculator\": res = Calculator(expression)\n",
    "            elif func_name.lower() == \"calendar\": res = Calendar()\n",
    "            results.append(res)\n",
    "            indexes.append(i)\n",
    "        except Exception: pass\n",
    "    prompts, start_idxs = [prompts[i] for i in indexes], [start_idxs[i] for i in indexes]\n",
    "    prompts_with_responses = format_api_calls(results, prompts, api_start_char, api_end_char, start_idxs=start_idxs)\n",
    "    return prompts_with_responses, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c624b9-7b7d-4a4b-8f3c-54ee15ddd235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: From this, we have 4 * 30 minutes = <% Calculator(4 * 30) → 120 %> 120 minutes. \n",
      "Output: From this, <% Calculator(4 * 30) → 120 %> we have 4 * 30 minutes =  120 minutes. \n",
      "Output: Today is the first <% Calendar() → Today is Thursday, July 20, 2023. %> Friday of the year.\n",
      "Output: The president of the United States is <% Calendar() → Today is Thursday, July 20, 2023. %> Joe Biden.\n"
     ]
    }
   ],
   "source": [
    "test_cases,i = make_api_calls(test_cases, api_start_char, api_end_char)\n",
    "for a in test_cases: print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd7e5e-6fa7-4ccb-923e-abf07c23c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_probs(token_ids, logits):\n",
    "    \"\"\"\n",
    "        Calculates a probability distribution over the vocabulary for each position\n",
    "        in the input sequence (predicts the next token), and for each position, returns \n",
    "        the probability of the next actual/observed token in the input sequence.\n",
    "    \"\"\"\n",
    "    logits = logits[:, :-1]\n",
    "    token_ids = token_ids[:, 1:]\n",
    "    token_ids = rearrange(token_ids, 'b n -> b n 1')\n",
    "    probs = logits.softmax(-1)\n",
    "    correct_token_id_pred_prob = probs.gather(-1, token_ids)\n",
    "    return rearrange(correct_token_id_pred_prob, 'b n 1 -> b n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a4f14f-8f98-4f66-ac71-f0771a6ab950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def weight_func(t): return (1. - t * 0.2).clamp(min=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9684ed26-ebb3-4716-b88a-a565dcc3855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_weights(tokens, search_token_id, pad_id=-1, weight_func=weight_func, start_index=None):\n",
    "    \"\"\"\n",
    "        Searches for the search_token_id in the sequence, and produces a \n",
    "        weight vector that degrades weighting off a cliff after the \n",
    "        search_token_id. Weights returned are equal for all tokens preceding\n",
    "        the search_token_id, and grade down to 0 over the next 5 tokens.\n",
    "    \"\"\"\n",
    "    # find the api_start_token\n",
    "    is_token_id_mask = torch.zeros_like(tokens, dtype=bool)\n",
    "    rows = torch.arange(is_token_id_mask.shape[0])\n",
    "    for i in range(len(tokens)):\n",
    "        idx = start_index[i]\n",
    "        is_token_id_mask[i,idx:] = (tokens[i,idx:] == search_token_id)\n",
    "    # generate a monotonic arange for all tokens after api_start_token\n",
    "    arange = (is_token_id_mask.cumsum(dim=-1) > 0).cumsum(dim=-1)\n",
    "    # set everything before the api_start_token to 0\n",
    "    before_token_mask = arange == 0\n",
    "    # set api_start_token to 0 in range\n",
    "    arange = arange - 1\n",
    "    # replace all before api_start_token with 0, so 0 up to api_start_token + 1\n",
    "    arange = arange.masked_fill(before_token_mask, pad_id)\n",
    "    # we now have a range like [0,0,0,0,0,0,0,(api_token)0,1,2,3,4,5...]\n",
    "    weights = weight_func(arange)\n",
    "    # now we have a weight vector like [1.2,1.2,1.2,1.2,1.2,1.2,1.2,(search_token_id)1,0.8,0.6,0.4,0.2,0,0,0,0...]\n",
    "    return weights.masked_fill(weights == pad_id, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df621437-94f7-4eed-94d5-77612ac40c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def toolformer_probability_filter(tokens_without_api_calls, tokens_with_api_calls, tokens_with_api_responses, api_start_token, api_end_token, tau_filter=1., start_idxs=None, device='cuda'):\n",
    "    # get the logits\n",
    "    def add_dims(x): return x[None, :] if len(x.shape) < 2 else x\n",
    "    \n",
    "    tokens_without_api_calls, tokens_with_api_calls, tokens_with_api_responses = map(lambda t: add_dims(t).to(device), (tokens_without_api_calls, tokens_with_api_calls, tokens_with_api_responses))\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        logits, logits_with_api_calls, logits_with_api_responses = map(partial(model, start_pos=0), (tokens_without_api_calls, tokens_with_api_calls, tokens_with_api_responses))\n",
    "    \n",
    "    # get the predicted probabilities\n",
    "    probs_without_api_calls = get_probs(tokens_without_api_calls, logits)\n",
    "    probs_with_api_calls = get_probs(tokens_with_api_calls, logits_with_api_calls)\n",
    "    probs_with_api_responses = get_probs(tokens_with_api_responses, logits_with_api_responses)\n",
    "    \n",
    "    # get the weightings\n",
    "    weights_without_api_calls = get_weights(tokens_with_api_calls[:, 1:], api_start_token, start_index=tensor(start_idxs))\n",
    "    weights_with_api_calls = get_weights(tokens_with_api_calls[:, :-1], api_end_token, start_index=tensor(start_idxs))\n",
    "    weights_with_api_responses = get_weights(tokens_with_api_responses[:, :-1], api_end_token, start_index=tensor(start_idxs))\n",
    "    \n",
    "    for w in weights_without_api_calls: assert w.sum() > 0\n",
    "    \n",
    "    # calculate the loss for each version\n",
    "    def loss(weights, probs): return -(weights * probs.log()).sum(-1)\n",
    "    loss_original = loss(weights_without_api_calls, probs_without_api_calls)\n",
    "    loss_api = loss(weights_with_api_calls, probs_with_api_calls)\n",
    "    loss_response = loss(weights_with_api_responses, probs_with_api_responses)\n",
    "\n",
    "    # toolformer filtering\n",
    "    l_minus = torch.minimum(loss_original, loss_api)\n",
    "    l_plus = loss_response\n",
    "    t_mask = (l_minus - l_plus) >= tau_filter\n",
    "    return tokens_without_api_calls[t_mask], tokens_with_api_calls[t_mask], tokens_with_api_responses[t_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8c0119-7c61-428b-8448-438ddf0c0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample(model, tokenizer, prompts: List[str], max_gen_len: int, temperature: float = 0.8, top_p: float = 0.95, decode=False, make_api_calls=False, device='cuda'):\n",
    "    bsz = len(prompts)\n",
    "    params = model.params\n",
    "    assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "    prompt_tokens = [tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "\n",
    "    min_prompt_size = min([len(t) for t in prompt_tokens])\n",
    "    max_prompt_size = max([len(t) for t in prompt_tokens])\n",
    "\n",
    "    total_len = min(params.max_seq_len, max_gen_len + max_prompt_size)\n",
    "    tokens = torch.full((bsz, total_len), tokenizer.pad_id).to(device).long()\n",
    "    for k, t in enumerate(prompt_tokens):\n",
    "        tokens[k, : len(t)] = torch.tensor(t).long()\n",
    "    input_text_mask = tokens != tokenizer.pad_id\n",
    "    start_pos = min_prompt_size\n",
    "    prev_pos = 0\n",
    "    \n",
    "    for cur_pos in range(start_pos, total_len):\n",
    "        logits = model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "            next_token = sample_top_p(probs, top_p)\n",
    "        else:\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "        next_token = next_token.reshape(-1)\n",
    "        # only replace token if prompt has already been generated\n",
    "        next_token = torch.where(\n",
    "            input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "        )\n",
    "        tokens[:, cur_pos] = next_token\n",
    "        prev_pos = cur_pos\n",
    "    return tokens if not decode else decode_tokens(tokenizer, tokens, prompt_tokens, max_gen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d178351-3d80-4304-8adc-77ce847eb6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad()\n",
    "def build_finetune_dataset(dataloader, model, tokenizer, api_start_char='<$', api_end_char='%>', return_tokens=True, device='cuda'):\n",
    "    \"\"\"\n",
    "        Samples API calls using in-context learning, and returns a dataset\n",
    "        that contains only examples for which calling the API increased the \n",
    "        model's ability to predict the next token.\n",
    "    \"\"\"\n",
    "    finetune_data = []\n",
    "    model = model.to(device)\n",
    "    api_start_token = tokenizer.encode(api_start_char, False, False)[0]\n",
    "    api_end_token = tokenizer.encode(api_end_char, False, False)[0]\n",
    "    for it, batch in enumerate(progress_bar(dataloader, leave=False)):\n",
    "\n",
    "        # assemble the null prompts assuming no API calls\n",
    "        prompts, (data_without_api_calls, start_idxs) = batch\n",
    "\n",
    "        data_without_api_calls = [p + d for p,d in zip(prompts, data_without_api_calls)]\n",
    "\n",
    "        # generate samples with possible API calls, and filter to a single API call per prompt\n",
    "        sampled_prompts = sample(model, tokenizer, prompts, max_gen_len=100, decode=True, device=device)\n",
    "        data_with_api_calls, indexes = filter_and_retain_only_first_api(sampled_prompts, api_start_char, api_end_char, start_idxs)\n",
    "        if len(data_with_api_calls) == 0: continue\n",
    "\n",
    "        # make the api calls\n",
    "        try: data_with_api_responses, indexes = make_api_calls(data_with_api_calls, api_start_char, api_end_char, indexes)\n",
    "        except Exception: continue\n",
    "        if len(data_with_api_responses) == 0: continue\n",
    "\n",
    "        # retain only data where we have a) without call, b) with call and c) with response\n",
    "        data_with_api_calls = [data_with_api_calls[i] for i in indexes]\n",
    "        data_without_api_calls = [data_without_api_calls[i] for i in indexes]\n",
    "\n",
    "        # convert to tokens and pad to same length\n",
    "        to_tokens = lambda l: pad_sequence(encode_to_tensor(tokenizer, l), batch_first=True)\n",
    "        tokens_without_api_calls, tokens_with_api_calls, tokens_with_api_responses = map(\n",
    "            to_tokens, (data_without_api_calls, data_with_api_calls, data_with_api_responses)\n",
    "        )\n",
    "        tokens_without_api_calls, tokens_with_api_calls, tokens_with_api_responses = torch.chunk(\n",
    "            pad_sequence(\n",
    "                [j for i in [tokens_without_api_calls, tokens_with_api_calls, tokens_with_api_responses] for j in i], \n",
    "                batch_first=True\n",
    "        ), 3, dim=0)\n",
    "\n",
    "        # filter data via the main toolformer equation\n",
    "        token_start_idxs = [encode_to_tensor(tokenizer, p).shape[-1] for p in prompts]\n",
    "        token_start_idxs = [token_start_idxs[i] for i in indexes]\n",
    "        finetune_tokens, finetune_tokens_with_api_calls, finetune_tokens_with_api_responses = toolformer_probability_filter(\n",
    "            tokens_without_api_calls, tokens_with_api_calls, tokens_with_api_responses, api_start_token, api_end_token, start_idxs=token_start_idxs, device=device\n",
    "        )\n",
    "\n",
    "        # store the relevant data\n",
    "        if len(finetune_tokens_with_api_calls) >= 1: \n",
    "            for f in finetune_tokens_with_api_calls: finetune_data.append(f.cpu())\n",
    "\n",
    "    if return_tokens: return finetune_data\n",
    "    prompts = []\n",
    "    for f in finetune_data:\n",
    "        l = [i.item() for i in f if not i == 0]\n",
    "        prompts.append(tokenizer.decode(l))\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e6885-405d-494f-a70e-c7d8320497fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37868c34-8ded-4cfc-8664-49eb5bb74424",
   "metadata": {},
   "source": [
    "### Test (7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb18712-55f9-48ca-9592-9758616c87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7bfa2-2b6a-4902-abb2-2fa0e6dd040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LOCAL_RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['MASTER_ADDR'] = '172.17.0.7'\n",
    "os.environ['MASTER_PORT'] = '6006'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68267304-5d0b-4f11-a8b0-35409a6726df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['layers.0.attention.inner_attention.rope.freqs', 'layers.1.attention.inner_attention.rope.freqs', 'layers.2.attention.inner_attention.rope.freqs', 'layers.3.attention.inner_attention.rope.freqs', 'layers.4.attention.inner_attention.rope.freqs', 'layers.5.attention.inner_attention.rope.freqs', 'layers.6.attention.inner_attention.rope.freqs', 'layers.7.attention.inner_attention.rope.freqs', 'layers.8.attention.inner_attention.rope.freqs', 'layers.9.attention.inner_attention.rope.freqs', 'layers.10.attention.inner_attention.rope.freqs', 'layers.11.attention.inner_attention.rope.freqs', 'layers.12.attention.inner_attention.rope.freqs', 'layers.13.attention.inner_attention.rope.freqs', 'layers.14.attention.inner_attention.rope.freqs', 'layers.15.attention.inner_attention.rope.freqs', 'layers.16.attention.inner_attention.rope.freqs', 'layers.17.attention.inner_attention.rope.freqs', 'layers.18.attention.inner_attention.rope.freqs', 'layers.19.attention.inner_attention.rope.freqs', 'layers.20.attention.inner_attention.rope.freqs', 'layers.21.attention.inner_attention.rope.freqs', 'layers.22.attention.inner_attention.rope.freqs', 'layers.23.attention.inner_attention.rope.freqs', 'layers.24.attention.inner_attention.rope.freqs', 'layers.25.attention.inner_attention.rope.freqs', 'layers.26.attention.inner_attention.rope.freqs', 'layers.27.attention.inner_attention.rope.freqs', 'layers.28.attention.inner_attention.rope.freqs', 'layers.29.attention.inner_attention.rope.freqs', 'layers.30.attention.inner_attention.rope.freqs', 'layers.31.attention.inner_attention.rope.freqs'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_rank, world_size = setup_model_parallel()\n",
    "path = '/home/models/foundation/LLaMA/7B'\n",
    "checkpoint = torch.load(f'{path}/consolidated.00.pth')\n",
    "with open(Path(path) / \"params.json\", \"r\") as f: params = json.loads(f.read())\n",
    "model_args = ModelArgs(max_seq_len=2048, max_batch_size=8, **params)\n",
    "model_args.vocab_size = tokenizer.n_words\n",
    "model = Transformer(model_args).cuda().half()\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383bee8-1640-451f-99db-bc109ad80f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "with open('../data/dataset.csv', 'r') as file: \n",
    "    reader = csv.reader(file)\n",
    "    for row in reader: d.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ceeac2-62de-487e-8740-b35cb6caa245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1168"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc6996-ec72-4daa-9a50-089464927583",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = PromptDS(d)\n",
    "dl = DataLoader(ds, batch_size=8, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f56dfd-ca1f-49f5-8394-d8afd0cd75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = [tokenizer.encode(p[0], True, True) for p in ds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab8b20-1ab9-40a0-bbf0-3382752d148c",
   "metadata": {},
   "source": [
    "(566,574)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bba52e-c39f-4ee3-8c08-4ad8e67c932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dds = []\n",
    "idxs = [i for i in range(0,8)]\n",
    "for i in idxs:\n",
    "    dds.append(ds[i])\n",
    "dds_prompts = [b[0] for b in dds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d218e-0fbe-4bee-8fc3-7e2f6de75812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/146 00:00&lt;? building dataset...]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_4261/763301092.py\u001b[0m(18)\u001b[0;36mbuild_finetune_dataset\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     16 \u001b[0;31m        \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_without_api_calls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_idxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     17 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 18 \u001b[0;31m        \u001b[0mdata_without_api_calls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_without_api_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     19 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     20 \u001b[0;31m        \u001b[0;31m# generate samples with possible API calls, and filter to a single API call per prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  prompts[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Your task is to add calls to a Calculator API to a piece of text. \\nThe calls should help you get information required to complete the text. \\nYou can call the API by writing \"<% Calculator(expression) %>\" where \"expression\" is the expression to be computed.\\nYou should simply return the same text with the API call included.\\nHere are some examples of API calls: \\nInput: The number in the next term is 18 + 12 x 3 = 54. \\nOutput: The number in the next term is 18 + 12 x 3 = <% Calculator(18 + 12 * 3) %> 54. \\nInput: The population is 658,893 people. This is 11.4% of the national average of 5,763,868 people. \\nOutput: The population is 658,893 people. This is 11.4% of the national average of <% Calculator(658,893 / 11.4) %> 5,763,868 people. \\nInput: A total of 252 qualifying matches were played, and 723 goals were scored (an average of 2.87 per match). This is three times less than the 2169 goals last year. \\nOutput: A total of 252 qualifying matches were played, and 723 goals were scored (an average of <% Calculator(723 / 252) %> 2.87 per match). This is twenty goals more than the <% Calculator(723 - 20) %> 703 goals last year. \\nInput: I went to Paris in 1994 and stayed there until 2011, so in total, it was 17 years. \\nOutput: I went to Paris in 1994 and stayed there until 2011, so in total, it was <% Calculator(2011 - 1994) %> 17 years. \\nInput: From this, we have 4 * 30 minutes = 120 minutes. \\nOutput: From this, we have 4 * 30 minutes = <% Calculator(4 * 30) %> 120 minutes. \\nInput: Janet sells 16.0 - 3.0 - 4.0 = 9.0 duck eggs a day. She makes 9.0 * 2.0 = $18.0 every day at the farmer’s market.  \\nOutput: '\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  data_without_api_calls[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Janet sells 16.0 - 3.0 - 4.0 = 9.0 duck eggs a day. She makes 9.0 * 2.0 = $18.0 every day at the farmer’s market. '\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  start_idxs[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1621)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_4261/763301092.py\u001b[0m(21)\u001b[0;36mbuild_finetune_dataset\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     19 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     20 \u001b[0;31m        \u001b[0;31m# generate samples with possible API calls, and filter to a single API call per prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 21 \u001b[0;31m        \u001b[0msampled_prompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_gen_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     22 \u001b[0;31m        \u001b[0mdata_with_api_calls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_and_retain_only_first_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_start_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_end_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     23 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_with_api_calls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  data_without_api_calls[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Your task is to add calls to a Calculator API to a piece of text. \\nThe calls should help you get information required to complete the text. \\nYou can call the API by writing \"<% Calculator(expression) %>\" where \"expression\" is the expression to be computed.\\nYou should simply return the same text with the API call included.\\nHere are some examples of API calls: \\nInput: The number in the next term is 18 + 12 x 3 = 54. \\nOutput: The number in the next term is 18 + 12 x 3 = <% Calculator(18 + 12 * 3) %> 54. \\nInput: The population is 658,893 people. This is 11.4% of the national average of 5,763,868 people. \\nOutput: The population is 658,893 people. This is 11.4% of the national average of <% Calculator(658,893 / 11.4) %> 5,763,868 people. \\nInput: A total of 252 qualifying matches were played, and 723 goals were scored (an average of 2.87 per match). This is three times less than the 2169 goals last year. \\nOutput: A total of 252 qualifying matches were played, and 723 goals were scored (an average of <% Calculator(723 / 252) %> 2.87 per match). This is twenty goals more than the <% Calculator(723 - 20) %> 703 goals last year. \\nInput: I went to Paris in 1994 and stayed there until 2011, so in total, it was 17 years. \\nOutput: I went to Paris in 1994 and stayed there until 2011, so in total, it was <% Calculator(2011 - 1994) %> 17 years. \\nInput: From this, we have 4 * 30 minutes = 120 minutes. \\nOutput: From this, we have 4 * 30 minutes = <% Calculator(4 * 30) %> 120 minutes. \\nInput: Janet sells 16.0 - 3.0 - 4.0 = 9.0 duck eggs a day. She makes 9.0 * 2.0 = $18.0 every day at the farmer’s market.  \\nOutput: Janet sells 16.0 - 3.0 - 4.0 = 9.0 duck eggs a day. She makes 9.0 * 2.0 = $18.0 every day at the farmer’s market. '\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  quit\n"
     ]
    }
   ],
   "source": [
    "data = build_finetune_dataset(dl, model, tokenizer, return_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39c64a-c4ad-424c-97fc-00df4f100038",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa08ee9-cff9-4713-86b2-7f262b20d612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[140107892849952, 94768956301296, 0,  ..., 140107892849952, 140107892849952, 197568495720],\n",
       "        [32, 48, 94768957192976,  ..., 80, 65, 94768956049216],\n",
       "        [94768956410192, 94768955679216, 1,  ..., 140107892849984, 140107892849984, -1],\n",
       "        ...,\n",
       "        [0, 7310593858020254331, 3616445622929465956,  ..., 94768955884784, -1, -1],\n",
       "        [140107892850000, 140107892850000, -1,  ..., 0, 0, 0],\n",
       "        [140107892850016, 94768956019856, 1,  ..., 94768956178224, 0, 94768956394992]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((8,680), -1).to('cuda').long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1476e58c-ec1e-42c4-a65d-54cda18fc44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_4970/252355549.py\u001b[0m(15)\u001b[0;36msample\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     13 \u001b[0;31m    \u001b[0mtotal_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_gen_len\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax_prompt_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 15 \u001b[0;31m    \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     16 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     17 \u001b[0;31m        \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_4970/252355549.py\u001b[0m(16)\u001b[0;36msample\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     14 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m    \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 16 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     17 \u001b[0;31m        \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     18 \u001b[0;31m    \u001b[0minput_text_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[140107892849952, 140107892849952, 468151435369,  ..., 450971566112, 137438953582, 472446402661],\n",
      "        [94768934595040, 94766974209360, 140106014609952,  ..., 94768934595040, 94766974209360, 140106014609632],\n",
      "        [0, 0, 0,  ..., 140107892850016, 94768515512032, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 94768956475408, 0, 94768934595040],\n",
      "        [94768956109072, 0, 94768934595040,  ..., 140107892850016, 94768956911072, 1],\n",
      "        [0, 0, 0,  ..., 94768955756000, 0, 94768934595040]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  quit\n"
     ]
    }
   ],
   "source": [
    "sample(model, tokenizer, dds_prompts, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6e073-7b86-4aeb-9be7-79498daf903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in data[:2]: print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff0e47-8eb9-4231-9c92-248a5913c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/libs/toolformer/data/finetune_dataset.csv', 'w', newline='') as file: \n",
    "    writer = csv.writer(file)\n",
    "    for d in data: writer.writerow(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "main_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
