{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a00878a-878d-40e8-838f-a8af0f4a1a2e",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Sample a finetuned model, hopefully with tool usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece55aa9-8985-427a-a8f4-8126dbf544f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1470a6-f32b-469b-ab49-844a27b97142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import math, random, torch, matplotlib.pyplot as plt, numpy as np, matplotlib as mpl, shutil, os, gzip, pickle, re, copy, time\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import fastcore.all as fc\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "from torch import tensor, nn, optim\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torch.nn import init\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import List, Optional\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "from fastprogress import progress_bar\n",
    "from einops import rearrange\n",
    "\n",
    "from toolformer.datasets import *\n",
    "from toolformer.tokenizer import *\n",
    "from toolformer.model import *\n",
    "from toolformer.tools import *\n",
    "from toolformer.filtering import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81974c6b-ac2e-4e3c-80a3-e8f76279d55d",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76edf1-068d-477b-9ea4-29ccc9723404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_finetuned_model(llama_path:str, tokenizer, local_rank:int, world_size:int, lora_path:str=None) -> Transformer:\n",
    "    checkpoints = sorted(Path(llama_path).glob(\"*.pth\"))\n",
    "    assert (\n",
    "        world_size == len(checkpoints)\n",
    "    ), f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {world_size}\"\n",
    "    ckpt_path = checkpoints[local_rank]\n",
    "    print(\"Loading...\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    \n",
    "    with open(Path(llama_path) / \"params.json\", \"r\") as f:\n",
    "        params = json.loads(f.read())\n",
    "    \n",
    "    model_args: ModelArgs = ModelArgs(max_seq_len=2048, max_batch_size=8, **params)\n",
    "    model_args.vocab_size = tokenizer.n_words\n",
    "    if lora_path: model_args.lora = True\n",
    "    torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "    model = Transformer(model_args).cuda().half()\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "    \n",
    "    model.load_state_dict(checkpoint, strict=False)\n",
    "    if lora_path: \n",
    "        lora_weights = torch.load(lora_path)\n",
    "        model.load_state_dict(lora_weights, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215762b-3a5c-4554-a500-abe3075d1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad()\n",
    "def sample_with_tools(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompts: List[str],\n",
    "    max_gen_len: int,\n",
    "    temperature: float = 0.8,\n",
    "    top_p: float = 0.95,\n",
    "    decode:bool=False,\n",
    "    device:str='cuda', \n",
    "    top_k:int=10,\n",
    "    api_start_token=20577,\n",
    "    api_end_token=6580\n",
    ") -> List[str]:\n",
    "    \n",
    "    bsz = len(prompts)\n",
    "    params = model.params\n",
    "    assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "    if make_api_calls: assert len(prompts) == 1\n",
    "\n",
    "    prompt_tokens = [tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "\n",
    "    min_prompt_size = min([len(t) for t in prompt_tokens])\n",
    "    max_prompt_size = max([len(t) for t in prompt_tokens])\n",
    "\n",
    "    total_len = min(params.max_seq_len, max_gen_len + max_prompt_size)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    tokens = torch.zeros(bsz, total_len).to(device).long()\n",
    "    for k, t in enumerate(prompt_tokens):\n",
    "        tokens[k, : len(t)] = torch.tensor(t).long()\n",
    "    input_text_mask = tokens != tokenizer.pad_id\n",
    "    start_pos = min_prompt_size\n",
    "    prev_pos = 0\n",
    "    inside_api_call = False\n",
    "    l = 0\n",
    "    \n",
    "    for cur_pos in range(start_pos, total_len):\n",
    "        logits = model.forward(tokens[:, prev_pos:cur_pos], prev_pos, last_logits_only=True)\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "            probs_topk = probs.topk(dim=-1, k=10)\n",
    "            if not inside_api_call and api_start_token in probs_topk.indices:\n",
    "                next_token = api_start_token\n",
    "                inside_api_call = True\n",
    "            else: next_token = sample_top_p(probs, top_p)  \n",
    "        else:\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "        next_token = next_token.reshape(-1)\n",
    "        # only replace token if prompt has already been generated\n",
    "        # next_token = torch.where(\n",
    "        #     input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "        # )\n",
    "        tokens[:, cur_pos] = next_token\n",
    "        if make_api_calls and inside_api_call and next_token == api_end_token:\n",
    "            prompt = \"\".join(self.decode(tokens))\n",
    "            prompt_with_api_responses = make_api_calls([prompt])\n",
    "            tokens_with_api_responses = toolformer.encode_to_tensor(prompt)\n",
    "            tokens[0,:len(tokens_with_api_responses)] = tokens_with_api_responses\n",
    "            l = len(tokens_with_api_responses) - cur_pos\n",
    "            inside_api_call = False\n",
    "        cur_pos = cur_pos + l\n",
    "        prev_pos = cur_pos\n",
    "\n",
    "    return tokens if not decode else decode_tokens(tokenizer, tokens, prompt_tokens, max_gen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4acac-bfa1-4f3f-bc49-ecbc70a209f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66b8920-d6ff-4d62-850a-19fe51a5afef",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e2bb1-4a07-44d9-93f4-6af054c84f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e08bc-fb67-470a-ab08-171dd15d9508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/models/foundation/LLaMA/tokenizer.model'\n",
    "# tokenizer = Tokenizer(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc1df35-c3c6-45d7-84b3-8a0c0fd389ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['LOCAL_RANK'] = '0'\n",
    "# os.environ['WORLD_SIZE'] = '1'\n",
    "# os.environ['RANK'] = '0'\n",
    "# os.environ['MASTER_ADDR'] = '172.17.0.3'\n",
    "# os.environ['MASTER_PORT'] = '6006'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed72c6-22b6-4efa-86d5-152af552f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_rank, world_size = setup_model_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a545acfc-5f01-4f1d-aaa1-9b5822851509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_path = '/home/models/foundation/LLaMA/7B/'\n",
    "# lora_path = '/home/libs/toolformer/models/toolformer_7b_weights_part_0.pth'\n",
    "# model = load_finetuned_model(llama_path, tokenizer, local_rank, world_size, lora_path=lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e132c-d445-4730-8744-68308366c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'What month will it be in 5 weeks time?'\n",
    "# sample_with_tools(model, tokenizer, [prompt], 100, decode=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "main_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
