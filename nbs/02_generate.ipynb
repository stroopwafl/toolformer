{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae5f43e-5c5d-4ef8-ae4b-5afd82c1ee25",
   "metadata": {},
   "source": [
    "# Text2Img\n",
    "\n",
    "Contains modules for doing basic text2img and img2img inference with stable diffusion models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f041de7-46dc-4e74-a2f9-dc6a33c1cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f23978-bd41-4b6f-a2a9-21f82a5b4c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers import LMSDiscreteScheduler\n",
    "from torch import tensor\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from IPython import display\n",
    "import fastcore.all as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f7c038-440d-44ad-91f7-e343c3c87a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_device():\n",
    "    if torch.cuda.is_available(): device = 'cuda'\n",
    "    else: device = 'cpu'\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e5435-1fb0-4a54-a8d9-be7910101eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def encode_text(prompt, device, tok=None, te=None):\n",
    "    if tok is not None: tokenizer = tok\n",
    "    if te is not None: text_encoder = te\n",
    "    tokens = tokenizer(\n",
    "        prompt, \n",
    "        padding=\"max_length\", \n",
    "        max_length=tokenizer.model_max_length, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    embedding = text_encoder(\n",
    "        tokens.input_ids.to(device)\n",
    "    )[0].half()\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868ec2ba-b041-494c-8b63-77d399437a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def encode_img(file_path, height, width, device, autoencoder=None):\n",
    "    if autoencoder is not None: vae = autoencoder\n",
    "    im = Image.open(file_path).resize((height, width))\n",
    "    im_tensor = transforms.ToTensor()(im).unsqueeze(0).to(device).half()\n",
    "    init_latents = 0.18215 * vae.encode(im_tensor * 2 - 1).latent_dist.sample()\n",
    "    return init_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a76b87e-6c6f-42ef-b047-a1d584324b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def decode_img(latent, autoencoder=None):\n",
    "    if autoencoder is not None: vae = autoencoder\n",
    "    with torch.no_grad():\n",
    "        img = vae.decode(1 / 0.18215 * latent).sample\n",
    "\n",
    "    img = (img / 2 + 0.5).clamp(0, 1)\n",
    "    img = img.detach().cpu().permute(0, 2, 3, 1).float().numpy()\n",
    "    img = (img * 255).round().astype(\"uint8\")\n",
    "    return Image.fromarray(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2527363-5ad0-493b-acfb-41261f875821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_diffusion_pipe(hub_path):\n",
    "    device = get_device()\n",
    "    unet = UNet2DConditionModel.from_pretrained(hub_path, subfolder=\"unet\", torch_dtype=torch.float16).to(device);\n",
    "    vae = AutoencoderKL.from_pretrained(hub_path, subfolder='vae', torch_dtype=torch.float16).to(device);\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(device);\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16);\n",
    "    return unet, vae, text_encoder, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7ccb3-a164-4f37-be6e-d462c73b176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_scheduler(beta_start=0.00085, beta_end=0.012, **kwargs):\n",
    "    return LMSDiscreteScheduler(\n",
    "        beta_start=beta_start, \n",
    "        beta_end=beta_end, \n",
    "        beta_schedule=\"scaled_linear\", \n",
    "        num_train_timesteps=1000,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5badd0-4507-4534-9340-e1361e1fd60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate(prompt, img_path, guidance_scale=7.5, seed=None, steps=70, height=512, width=512, start_step=10, return_latents=False, device='cuda', autoencoder=None, uunet=None, tok=None, sched=None, te=None):\n",
    "    \n",
    "    if uunet is not None: unet = uunet\n",
    "    if sched is not None: scheduler = sched\n",
    "    text = encode_text(prompt, device, tok=tok, te=te)\n",
    "    uncond = encode_text(\"\", device, tok=tok, te=te)\n",
    "    text_embeddings = torch.cat([uncond, text])\n",
    "    \n",
    "    if seed: torch.manual_seed(seed)\n",
    "    \n",
    "    scheduler.set_timesteps(steps)\n",
    "    \n",
    "    if img_path is not None:\n",
    "        init_latents = encode_img(img_path, height, width, device, autoencoder=autoencoder)\n",
    "        noise = torch.randn([1, unet.in_channels, height//8, width//8]).to(device)\n",
    "        latents = scheduler.add_noise(\n",
    "            init_latents, \n",
    "            noise, \n",
    "            timesteps=torch.tensor([scheduler.timesteps[start_step]])\n",
    "        ).to(device).half()\n",
    "    else: \n",
    "        latents = torch.randn(1, unet.in_channels, height//8, width//8).to(device).half()\n",
    "        latents = latents * scheduler.init_noise_sigma\n",
    "    \n",
    "    for i, ts in enumerate(tqdm(scheduler.timesteps)):\n",
    "        \n",
    "        if i > start_step:\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            latent_model_input = scheduler.scale_model_input(latent_model_input, ts)\n",
    "        \n",
    "            with torch.no_grad(): \n",
    "                noise_pred = unet(latent_model_input, ts, encoder_hidden_states=text_embeddings).sample\n",
    "                \n",
    "            u, t = noise_pred.chunk(2)\n",
    "            noise_pred = u + guidance_scale * (t - u)\n",
    "            latents = scheduler.step(noise_pred, ts, latents).prev_sample\n",
    "        \n",
    "    image = decode_img(latents, autoencoder=autoencoder)\n",
    "    \n",
    "    if return_latents: return image, latents\n",
    "    else: return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09934146-2ffd-4c0c-b1c2-b5c0c9e6723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_clip(embed_path, text_encoder, tokenizer, token=None):\n",
    "    loaded_learned_embeds = torch.load(embed_path, map_location=\"cpu\")\n",
    "\n",
    "    # separate token and the embeds\n",
    "    trained_token = list(loaded_learned_embeds.keys())[0]\n",
    "    embeds = loaded_learned_embeds[trained_token]\n",
    "\n",
    "    # cast to dtype of text_encoder\n",
    "    dtype = text_encoder.get_input_embeddings().weight.dtype\n",
    "    embeds.to(dtype)\n",
    "\n",
    "    # add the token in tokenizer\n",
    "    token = token if token is not None else trained_token\n",
    "    num_added_tokens = tokenizer.add_tokens(token)\n",
    "    if num_added_tokens == 0:\n",
    "        raise ValueError(f\"The tokenizer already contains the token {token}. Please pass a different `token` that is not already in the tokenizer.\")\n",
    "\n",
    "    # resize the token embeddings\n",
    "    text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # get the id for the token and assign the embeds\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    text_encoder.get_input_embeddings().weight.data[token_id] = embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653eadc8-0af3-44ef-9332-5bd97dbc110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Wizard:\n",
    "    def __init__(self, model_path, embeddings:list=None):\n",
    "        self.unet, self.vae, self.text_encoder, self.tokenizer = get_diffusion_pipe(model_path)\n",
    "        self.scheduler = get_scheduler()\n",
    "        self.device = get_device()\n",
    "        if embeddings is not None:\n",
    "            for e in embeddings: embed_clip(e, self.text_encoder, self.tokenizer)\n",
    "    def gen(self, prompt, img_path=None, guidance_scale=7.5, seed=None, steps=70, height=512, width=512, start_step=0, return_latents=False):\n",
    "        if img_path is not None and start_step == 0: start_step = 10\n",
    "        gen = generate(\n",
    "            prompt, \n",
    "            img_path=img_path, \n",
    "            guidance_scale=guidance_scale, \n",
    "            seed=seed, \n",
    "            steps=steps, \n",
    "            height=height, \n",
    "            width=width, \n",
    "            start_step=start_step, \n",
    "            return_latents=return_latents, \n",
    "            device=self.device,\n",
    "            sched=self.scheduler,\n",
    "            autoencoder=self.vae,\n",
    "            tok=self.tokenizer,\n",
    "            uunet=self.unet,\n",
    "            te=self.text_encoder\n",
    "        )\n",
    "        if return_latents: \n",
    "            img, latents = gen\n",
    "            return img, latents\n",
    "        else: return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a60dc5-5993-4067-a009-1ad57832b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
