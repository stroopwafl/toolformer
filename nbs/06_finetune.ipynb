{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d56ed84-b717-46c8-a67d-ab39702f635e",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f09caf-19c2-440b-aff0-3b68e3aa5345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7796f9e-aeec-47bf-8800-2a7e8903c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import math, random, torch, matplotlib.pyplot as plt, numpy as np, matplotlib as mpl, shutil, os, gzip, pickle, re, copy, time\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import fastcore.all as fc\n",
    "from glob import glob\n",
    "\n",
    "from torch import tensor, nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torch.nn import init\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from toolformer.model import *\n",
    "from toolformer.tokenizer import *\n",
    "from fastprogress import progress_bar\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e48206-b3a6-461d-8b56-d82359f1b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_gen(l):\n",
    "    for i in l: yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b404bc-4bfd-4e7d-960f-069ef16f5d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def set_grads(model, set_grads_to=False, lora=False):\n",
    "    if lora:\n",
    "        for name, mod in model.named_modules():\n",
    "            for p in mod.parameters():\n",
    "                p.requires_grad = set_grads_to if 'lora' in name else False\n",
    "    else:\n",
    "        for p in model.parameters(): p.requires_grad = set_grads_to\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430b8bf9-d97a-4c2e-81c9-872f1beb8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_model_weights(save_path, model, lora=False):\n",
    "    if lora:\n",
    "        keys = [k for k in model.state_dict() if 'lora' in k]\n",
    "        params = [model.state_dict()[key] for key in keys]\n",
    "        d = {k:p for k,p in zip(keys, params)}\n",
    "        torch.save(d, save_path)\n",
    "    else: torch.save(model.state_dict(), save_path)\n",
    "    print(f'Saved model weights at {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2103bc6-33eb-41fd-913b-d95606f4de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_lora_weights(path, model):\n",
    "    weights = torch.load(path)\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43781347-ebd1-40b1-a45b-6c94e5c2481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def finetune(model, dataset, save_path, lr=1e-5, epochs=10, bs=1, opt_func=partial(optim.Adam, eps=1e-5), lora=True, device='cuda'):\n",
    "    assert len(dataset) > 0\n",
    "    dl = DataLoader(dataset, batch_size=bs, shuffle=True, collate_fn=partial(pad_sequence, batch_first=True), num_workers=4)\n",
    "\n",
    "    model.train()\n",
    "    set_grads(model, set_grads_to=True, lora=lora)\n",
    "    opt = opt_func([p for p in model.parameters() if p.requires_grad], lr)\n",
    "    \n",
    "    for epoch in progress_bar(range(epochs), comment='finetuning...'):\n",
    "        for i, batch in enumerate(progress_bar(dl, leave=False)):\n",
    "            inp, label = batch[:,:-1].to(device), batch[:,1:].to(device)\n",
    "            logits = model(inp, 0)\n",
    "            print(logits.isnan().float().mean())\n",
    "            logits = rearrange(logits, 'b s v -> b v s')\n",
    "            loss = F.cross_entropy(logits, label, ignore_index=tokenizer.pad_id)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "    set_grads(model, set_grads_to=False, lora=lora)\n",
    "    save_model_weights(save_path, model, lora=lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5a92b-1cfc-4796-969f-18fb80cb92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FinetuneDS:\n",
    "    def __init__(self, prompts:List[str]): fc.store_attr()\n",
    "    def __len__(self): return len(self.prompts)\n",
    "    def __getitem__(self, i): \n",
    "        prompt = self.prompts[i]\n",
    "        tokens = tokenizer.encode(prompt, bos=True, eos=True)\n",
    "        return torch.tensor(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ccbcff-b64e-43d0-8da5-92745a90d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78db187-040e-4375-ac53-8f3c7f0e5c6d",
   "metadata": {},
   "source": [
    "### Test (7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993fed7d-ce68-4514-a333-a6c7b87da71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b71ac60-495f-4d2a-be14-aedd1f3e80c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/models/foundation/LLaMA/tokenizer.model'\n",
    "# tokenizer = Tokenizer(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2192646-0c38-45a8-80fc-ec187b1d2fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['LOCAL_RANK'] = '0'\n",
    "# os.environ['WORLD_SIZE'] = '1'\n",
    "# os.environ['RANK'] = '0'\n",
    "# os.environ['MASTER_ADDR'] = '172.17.0.3'\n",
    "# os.environ['MASTER_PORT'] = '6006'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd93047-a2f8-493c-a73d-26c2677b6f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_rank, world_size = setup_model_parallel()\n",
    "# path = '/home/models/foundation/LLaMA/7B'\n",
    "# checkpoint = torch.load(f'{path}/consolidated.00.pth')\n",
    "# with open(Path(path) / \"params.json\", \"r\") as f: params = json.loads(f.read())\n",
    "# model_args = ModelArgs(max_seq_len=2048, max_batch_size=8, **params)\n",
    "# model_args.vocab_size = tokenizer.n_words\n",
    "# model_args.lora = True\n",
    "# model = Transformer(model_args).cuda().half()\n",
    "# torch.set_default_tensor_type(torch.FloatTensor)\n",
    "# model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dde69d-e459-456b-a5d2-c87077516e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = []\n",
    "# with open('../data/example_finetune_dataset.csv', 'r') as file: \n",
    "#     reader = csv.reader(file)\n",
    "#     for row in reader: d.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d984d-9fd9-4d22-8883-b7c2563da03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = FinetuneDS(d)\n",
    "# len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d481ee2-b395-4821-80d4-1b0f21bf2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = f'../models/toolformer_7b_weights_part_{torch.distributed.get_rank()}.pth'\n",
    "# finetune(model, ds, save_path, lr=1e-5, opt_func=partial(optim.Adam, eps=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571ffd9-fc83-4ad8-92d8-c25277733275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "main_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
