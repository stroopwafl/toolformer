[
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "Inference",
    "section": "",
    "text": "Load model\n\nsource\n\n\nload_finetuned_model\n\n load_finetuned_model (llama_path:str, tokenizer, local_rank:int,\n                       world_size:int, lora_path:str=None)\n\n\nsource\n\n\nsample_with_tools\n\n sample_with_tools (model, tokenizer, prompts:List[str], max_gen_len:int,\n                    temperature:float=0.8, top_p:float=0.95,\n                    decode:bool=False, device:str='cuda', top_k:int=10,\n                    api_start_token=20577, api_end_token=6580)\n\n\n\nTest\n\n# import json\n\n\n# path = '/home/models/foundation/LLaMA/tokenizer.model'\n# tokenizer = Tokenizer(path)\n\n\n# os.environ['LOCAL_RANK'] = '0'\n# os.environ['WORLD_SIZE'] = '1'\n# os.environ['RANK'] = '0'\n# os.environ['MASTER_ADDR'] = '172.17.0.3'\n# os.environ['MASTER_PORT'] = '6006'\n\n\n# local_rank, world_size = setup_model_parallel()\n\n\n# llama_path = '/home/models/foundation/LLaMA/7B/'\n# lora_path = '/home/libs/toolformer/models/toolformer_7b_weights_part_0.pth'\n# model = load_finetuned_model(llama_path, tokenizer, local_rank, world_size, lora_path=lora_path)\n\n\n# prompt = 'What month will it be in 5 weeks time?'\n# sample_with_tools(model, tokenizer, [prompt], 100, decode=True)"
  },
  {
    "objectID": "tokenizer.html",
    "href": "tokenizer.html",
    "title": "Tokenizer",
    "section": "",
    "text": "source\n\nTokenizer\n\n Tokenizer (model_path:str)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nencode_to_tensor\n\n encode_to_tensor (tokenizer, prompt:[typing.List[str],&lt;class'str'&gt;],\n                   eos=True)\n\n\nsource\n\n\ndecode_tokens\n\n decode_tokens (tokenizer, tokens, prompt_tokens, max_gen_len)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "unstably-diffused",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "unstably-diffused",
    "section": "Install",
    "text": "Install\npip install unstably_diffused"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "unstably-diffused",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "Tools",
    "section": "",
    "text": "source\n\nCalendar\n\n Calendar ()\n\n\nCalendar()\n\n'Today is Thursday, July 20, 2023.'\n\n\n\nsource\n\n\nCalculator\n\n Calculator (expression:str)\n\n\nCalculator('12x3+8')\n\n44"
  },
  {
    "objectID": "toolformer.html",
    "href": "toolformer.html",
    "title": "Toolformer - dataset filtering",
    "section": "",
    "text": "We are going to use in-context learning to finetune the model. We’ll start with a prompt that teaches the model how to use a tool, and build a dataset of examples which vary the final input value inside this prompt. This first involves choosing a token to represent the beginning and end of an instance of tool usage.\nThrough trial and error, I chose “&lt;%” and “%&gt;” because these were the shortest tokens I could find that were a) represented by a single token, b) represented only once in the vocabulary (i.e. there are no duplicates) and c) unlikely to come up otherwise.\n\n# path = '/home/models/foundation/LLaMA/tokenizer.model'\n# tokenizer = Tokenizer(path)\n\n\n# p = ['{$', '{:', '!&gt;', '&lt;!', '&lt;%', '%&gt;']\n\n# for a in p:\n#     counter = 0\n#     for i in range(32000):\n#         t = tokenizer.decode(i)\n#         if t == a: counter += 1\n#     print(f'{a} : {counter}')\n\n\n# tokenizer.encode('&lt;%', False, False), tokenizer.encode('%&gt;', False, False)\n\n\ntest_cases = [\n    'Output: The number in the next term is 18 + 12 x 3 = 54.',\n    'Output: From this, we have 4 * 30 minutes = &lt;% Calculator(4 * 30) %&gt; 120 minutes. ',\n    'Output: From this, &lt;% Calculator(4 * 30) %&gt; we have 4 * 30 minutes = &lt;% Calculator(3 * 50) %&gt; 120 minutes. &lt;% Calculator(9 * 80) %&gt;',\n    'Output: Today is the first &lt;% Calendar() %&gt; Friday of the year.',\n    'Output: The president of the United States is &lt;% Calendar() %&gt; Joe Biden.'\n]\n\n\nsource\n\nfilter_and_retain_only_first_api\n\n filter_and_retain_only_first_api (prompts:List[str], api_start_char:str,\n                                   api_end_char:str,\n                                   start_idxs:Optional[List[int]]=None)\n\nTakes a list of strings and attempts to find instances of API calls in the pattern  api(expression) , and returns the original strings where only the first instance of this pattern remains (all others are replaced with ““). If no matches are found, discards the string.\n\n# api_start_char, api_end_char = '&lt;%', '%&gt;'\n# test_cases, i = filter_and_retain_only_first_api([test_cases], api_start_char, api_end_char)\n# # for a in test_cases: print(a)\n\n\nsource\n\n\nformat_api_calls\n\n format_api_calls (results, prompts, api_start_char:str, api_end_char:str,\n                   start_idxs:List[int]=None)\n\n\nsource\n\n\nmake_api_calls\n\n make_api_calls (prompts:List[str], api_start_char:str, api_end_char:str,\n                 start_idxs:List[int]=None)\n\nExtracts a calculator API call in the format  api(expression)  from a string, executes the API call and returns a new string including a response in the format  api(expression) → response .\n\n\nExample\ninput: ‘The number in the next term is 18 + 12 x 3 = &lt;&lt; Calculator(18 + 12 x 3) &gt;&gt; 54.’ output: ‘The number in the next term is 18 + 12 x 3 = &lt;&lt; Calculator(18 + 12 x 3) → 54 &gt;&gt; 54.’\n\n# test_cases,i = make_api_calls(test_cases, api_start_char, api_end_char)\n# for a in test_cases: print(a)\n\n\nsource\n\n\nget_probs\n\n get_probs (token_ids, logits)\n\nCalculates a probability distribution over the vocabulary for each position in the input sequence (predicts the next token), and for each position, returns the probability of the next actual/observed token in the input sequence.\n\nsource\n\n\nweight_func\n\n weight_func (t)\n\n\nsource\n\n\nget_weights\n\n get_weights (tokens, search_token_id, pad_id=-1, weight_func=&lt;function\n              weight_func&gt;, start_index=None)\n\nSearches for the search_token_id in the sequence, and produces a weight vector that degrades weighting off a cliff after the search_token_id. Weights returned are equal for all tokens preceding the search_token_id, and grade down to 0 over the next 5 tokens.\n\nsource\n\n\ntoolformer_probability_filter\n\n toolformer_probability_filter (tokens_without_api_calls,\n                                tokens_with_api_calls,\n                                tokens_with_api_responses,\n                                api_start_token, api_end_token,\n                                tau_filter=1.0, start_idxs=None,\n                                device='cuda')\n\n\nsource\n\n\nsample\n\n sample (model, tokenizer, prompts:List[str], max_gen_len:int,\n         temperature:float=0.8, top_p:float=0.95, decode=False,\n         make_api_calls=False, device='cuda')\n\n\nsource\n\n\nbuild_finetune_dataset\n\n build_finetune_dataset (dataloader, model, tokenizer,\n                         api_start_char='&lt;%', api_end_char='%&gt;',\n                         return_tokens=True, device='cuda')\n\nSamples API calls using in-context learning, and returns a dataset that contains only examples for which calling the API increased the model’s ability to predict the next token.\n\n\nTest (7B)\n\n# import json, csv\n\n\n# os.environ['LOCAL_RANK'] = '0'\n# os.environ['WORLD_SIZE'] = '1'\n# os.environ['RANK'] = '0'\n# os.environ['MASTER_ADDR'] = '172.17.0.3'\n# os.environ['MASTER_PORT'] = '6006'\n\n\n# local_rank, world_size = setup_model_parallel()\n# path = '/home/models/foundation/LLaMA/7B'\n# checkpoint = torch.load(f'{path}/consolidated.00.pth')\n# with open(Path(path) / \"params.json\", \"r\") as f: params = json.loads(f.read())\n# model_args = ModelArgs(max_seq_len=2048, max_batch_size=8, **params)\n# model_args.vocab_size = tokenizer.n_words\n# model = Transformer(model_args).cuda().half()\n# torch.set_default_tensor_type(torch.FloatTensor)\n# model.load_state_dict(checkpoint, strict=False)\n\n\n# d = []\n# with open('../data/dataset.csv', 'r') as file: \n#     reader = csv.reader(file)\n#     for row in reader: d.append(row)\n\n\n# ds = PromptDS(d)\n# dl = DataLoader(ds, batch_size=8, num_workers=4)\n\n\n# data = build_finetune_dataset(dl, model, tokenizer, return_tokens=False)\n\n\n# with open('/home/libs/toolformer/data/example_finetune_dataset.csv', 'w', newline='') as file: \n#     writer = csv.writer(file)\n#     for d in data: writer.writerow([d])"
  },
  {
    "objectID": "finetune.html",
    "href": "finetune.html",
    "title": "Finetuning",
    "section": "",
    "text": "source\n\nget_gen\n\n get_gen (l)\n\n\nsource\n\n\nset_grads\n\n set_grads (model, set_grads_to=False, lora=False)\n\n\nsource\n\n\nsave_model_weights\n\n save_model_weights (save_path, model, lora=False)\n\n\nsource\n\n\nload_lora_weights\n\n load_lora_weights (path, model)\n\n\nsource\n\n\nfinetune\n\n finetune (model, dataset, save_path, lr=1e-05, epochs=10, bs=1,\n           opt_func=functools.partial(&lt;class 'torch.optim.adam.Adam'&gt;,\n           eps=1e-05), lora=True, device='cuda')\n\n\nsource\n\n\nFinetuneDS\n\n FinetuneDS (prompts:List[str])\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nTest (7B)\n\n# import json, csv\n\n\n# path = '/home/models/foundation/LLaMA/tokenizer.model'\n# tokenizer = Tokenizer(path)\n\n\n# os.environ['LOCAL_RANK'] = '0'\n# os.environ['WORLD_SIZE'] = '1'\n# os.environ['RANK'] = '0'\n# os.environ['MASTER_ADDR'] = '172.17.0.3'\n# os.environ['MASTER_PORT'] = '6006'\n\n\n# local_rank, world_size = setup_model_parallel()\n# path = '/home/models/foundation/LLaMA/7B'\n# checkpoint = torch.load(f'{path}/consolidated.00.pth')\n# with open(Path(path) / \"params.json\", \"r\") as f: params = json.loads(f.read())\n# model_args = ModelArgs(max_seq_len=2048, max_batch_size=8, **params)\n# model_args.vocab_size = tokenizer.n_words\n# model_args.lora = True\n# model = Transformer(model_args).cuda().half()\n# torch.set_default_tensor_type(torch.FloatTensor)\n# model.load_state_dict(checkpoint, strict=False)\n\n\n# d = []\n# with open('../data/example_finetune_dataset.csv', 'r') as file: \n#     reader = csv.reader(file)\n#     for row in reader: d.append(row[0])\n\n\n# ds = FinetuneDS(d)\n# len(ds)\n\n\n# save_path = f'../models/toolformer_7b_weights_part_{torch.distributed.get_rank()}.pth'\n# finetune(model, ds, save_path, lr=1e-5, opt_func=partial(optim.Adam, eps=1e-5))"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nplot_images\n\n plot_images (images, rows=1, columns=5)\n\n\nsource\n\n\nget_device\n\n get_device ()"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Model",
    "section": "",
    "text": "This is just a slightly adapted version of the original LLaMA 1 code available at this repo.\n\nsource\n\nModelArgs\n\n ModelArgs (dim:int=512, n_layers:int=8, n_heads:int=8, vocab_size:int=-1,\n            multiple_of:int=256, norm_eps:float=1e-05,\n            max_batch_size:int=32, max_seq_len:int=2048)\n\n\nsource\n\n\nRMSNorm\n\n RMSNorm (dim:int, eps:float=1e-06)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nprecompute_freqs_cis\n\n precompute_freqs_cis (dim:int, end:int, theta:float=10000.0)\n\n\nsource\n\n\nreshape_for_broadcast\n\n reshape_for_broadcast (freqs_cis:torch.Tensor, x:torch.Tensor)\n\n\nsource\n\n\napply_rotary_emb\n\n apply_rotary_emb (xq:torch.Tensor, xk:torch.Tensor,\n                   freqs_cis:torch.Tensor)\n\n\nsource\n\n\nLoRA\n\n LoRA (input_dim, output_dim)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nAttention\n\n Attention (args:__main__.ModelArgs)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nFeedForward\n\n FeedForward (dim:int, hidden_dim:int, multiple_of:int)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nTransformerBlock\n\n TransformerBlock (layer_id:int, args:__main__.ModelArgs)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nTransformer\n\n Transformer (params:__main__.ModelArgs)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nsample_top_p\n\n sample_top_p (probs, p)\n\n\nsource\n\n\nsetup_model_parallel\n\n setup_model_parallel ()"
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Calendar\nA proof-of-concept dataset for the calendar API can be produced via the method described in the paper.\n\nsource\n\n\nrandom_date\n\n random_date (start, end)\n\n\nsource\n\n\nformat_date\n\n format_date (date)\n\n\nsource\n\n\nget_prompt\n\n get_prompt (template, holidays)\n\n\nsource\n\n\nmake_dateset\n\n make_dateset (templates, holidays, size=9400)\n\n\ncalendar_ds = make_dateset(DATESET_TEMPLATES, DATESET_HOLIDAYS, 600)\ncalendar_ds[:10]\n\n['How many days ago was Saturday, March 12, 1988?',\n 'How many days ago was Tuesday, September 12, 1893?',\n 'How many days are there until Friday, December 3, 2106?',\n 'How many days ago was Sunday, October 23, 2011?',\n 'How many days are there until Tuesday, April 13, 2190?',\n 'How many days are there until Saturday, April 5, 2053?',\n 'How many days are there until Friday, October 14, 2135?',\n 'How many days are there until Friday, May 15, 2054?',\n 'How many days ago was Monday, December 4, 1882?',\n 'How many days ago was Friday, March 27, 1767?']\n\n\n\nprint(CALENDAR_PROMPT.replace('[INPUT]', calendar_ds[0]))\n\nYour task is to add calls to a Calendar API to a piece of text. \nThe API calls should help you get information required to complete the text. \nYou can call the API by writing &lt;% Calendar() %&gt;. \nHere are some examples of API calls:\nInput: Today is the first Friday of the year.\nOutput: Today is the first &lt;% Calendar() %&gt; Friday of the year.\nInput: The president of the United States is Joe Biden.\nOutput: The president of the United States is &lt;% Calendar() %&gt; Joe Biden.\nInput: The current day of the week is Wednesday.\nOutput: The current day of the week is &lt;% Calendar() %&gt; Wednesday.\nInput: The number of days from now until Christmas is 30.\nOutput: The number of days from now until Christmas is &lt;% Calendar() %&gt; 30.\nInput: The store is never open on the weekend, so today it is closed.\nOutput: The store is never open on the weekend, so today &lt;% Calendar() %&gt; it is closed.\nInput: How many days ago was Saturday, March 12, 1988?\nOutput: \n\n\n\n\nCalculator\nI’m going to use a subset of the GSM9K-XL dataset.\n\nwith open('../data/gsm8k.json', 'r') as f: \n    qs = f.readlines()\n    test_data = [json.loads(q) for q in qs]\n\n\nraw_answers = [i[\"answer\"] for i in test_data]\nvalues = [i['v'] for i in test_data]\nresults = [i['r'] for i in test_data]\ncalculator_ds = []\nfor i, (v, r, a) in enumerate(zip(values, results, raw_answers)):\n    for j in range(len(v)):\n        a = a.replace(f\"{{v_{j+1}}}\", str(v[j]))\n    for j in range(len(r)):\n        a = a.replace(f\"{{r_{j+1}}}\", str(r[j]))\n    a = re.sub('&lt;&lt;.*?&gt;&gt;', '', a)\n    a = a.split(\"####\", 1)[0]\n    a = a.replace(\"\\n\", \" \")\n    calculator_ds.append(a)\n\n\ncalculator_ds[0]\n\n'Janet sells 16.0 - 3.0 - 4.0 = 9.0 duck eggs a day. She makes 9.0 * 2.0 = $18.0 every day at the farmer’s market. '\n\n\n\nprint(CALCULATOR_PROMPT.replace('[INPUT]', calculator_ds[0]))\n\nYour task is to add calls to a Calculator API to a piece of text. \nThe calls should help you get information required to complete the text. \nYou can call the API by writing \"&lt;% Calculator(expression) %&gt;\" where \"expression\" is the expression to be computed.\nYou should simply return the same text with the API call included.\nHere are some examples of API calls: \nInput: The number in the next term is 18 + 12 x 3 = 54. \nOutput: The number in the next term is 18 + 12 x 3 = &lt;% Calculator(18 + 12 * 3) %&gt; 54. \nInput: The population is 658,893 people. This is 11.4% of the national average of 5,763,868 people. \nOutput: The population is 658,893 people. This is 11.4% of the national average of &lt;% Calculator(658,893 / 11.4) %&gt; 5,763,868 people. \nInput: A total of 252 qualifying matches were played, and 723 goals were scored (an average of 2.87 per match). This is three times less than the 2169 goals last year. \nOutput: A total of 252 qualifying matches were played, and 723 goals were scored (an average of &lt;% Calculator(723 / 252) %&gt; 2.87 per match). This is twenty goals more than the &lt;% Calculator(723 - 20) %&gt; 703 goals last year. \nInput: I went to Paris in 1994 and stayed there until 2011, so in total, it was 17 years. \nOutput: I went to Paris in 1994 and stayed there until 2011, so in total, it was &lt;% Calculator(2011 - 1994) %&gt; 17 years. \nInput: From this, we have 4 * 30 minutes = 120 minutes. \nOutput: From this, we have 4 * 30 minutes = &lt;% Calculator(4 * 30) %&gt; 120 minutes. \nInput: Janet sells 16.0 - 3.0 - 4.0 = 9.0 duck eggs a day. She makes 9.0 * 2.0 = $18.0 every day at the farmer’s market.  \nOutput: \n\n\n\n\nExport and wrap into dataset\nThe dataset will return the full prompt, including the in-context learning prefix, the varying input alone, and also the start index of the models’ response. This will make life easier in the forthcoming dataset builder.\n\nsource\n\n\nPromptDS\n\n PromptDS (data)\n\nReturns a tuple containing the whole prompt (including the in-context teacher prefix), the varying input data sequence and the start index of the model’s response."
  }
]